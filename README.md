# ðŸ§  Bilde-segmentering med DINOv3 og logistisk regresjon

Dette prosjektet er en del av arbeidskravet i **Anvendt maskinlÃ¦ring** ved Viken Fagskole.  
FormÃ¥let er Ã¥ forstÃ¥ hvordan en forhÃ¥ndstrent nevralt nettverk (DINOv3 fra Meta AI) kan brukes til **bilde-segmentering** uten Ã¥ trene et stort nettverk fra bunnen av.

---

## ðŸŽ¯ MÃ¥l
1. Bruke DINOv3 (ViT-S/16) som **feature extractor** for Ã¥ hente ut egenskaper fra bilder.  
2. Trene en **logistisk regresjonsmodell (clf)** pÃ¥ toppen av disse feature-vektorene.  
3. Bruke modellen til Ã¥ **segmentere nye bilder** og vise resultatene.  
4. (Valgfritt) teste modellen pÃ¥ en **video** i sanntid for Ã¥ demonstrere generalisering.

---

## ðŸ“ Mappestruktur
```
Segmentering/
â”‚
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ images/        â†’ Inneholder treningsbilder
â”‚   â””â”€â”€ labels/        â†’ Inneholder maskebilder (labels)
â”‚
â”œâ”€â”€ notebooks/
â”‚    â””â”€â”€ foreground_segmentation.ipynb
â”œâ”€â”€ tools/
â”‚    â””â”€â”€ click_n_mask.py
â”‚    â””â”€â”€ sam_vit_b_01ec64.pth
â”œâ”€â”€ Videoer/           â†’ Valgfri mappe for Ã¥ vise video med AI-segmentering
â”‚    â””â”€â”€ test.mp4
â”œâ”€â”€ dinov3_vits16.pth  â†’ ForhÃ¥ndstrente vekter (lastes ned automatisk om den mangler)
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Installasjon

### 1. Opprett og aktiver miljÃ¸ (valgfritt)
```bash
python -m venv venv
venv\Scripts\activate
```

### 2. Installer nÃ¸dvendige pakker
```bash
pip install -r requirements.txt
```

### 3. Start Jupyter Notebook
```bash
jupyter notebook
```

---

## ðŸ§© Hoveddeler av oppgaven

### **1. Laste inn bilder og masker**
Bilder hentes fra Data/images/, og tilhÃ¸rende masker fra Data/labels/.
Hvert bilde har en binÃ¦r maske hvor hvitt representerer verktÃ¸y og svart bakgrunn.

Maskene er laget med vÃ¥rt eget program click_n_mask.py, som bruker Segment Anything (SAM).
Brukeren klikker pÃ¥ verktÃ¸yet (positive punkter) og bakgrunnen (negative punkter), og programmet genererer en ferdig maske.

### **2. Feature-ekstraksjon med DINOv3**
Vi bruker DINOv3 (Vision Transformer â€“ ViT-S/16) til Ã¥ hente ut representasjoner av bildene.
Disse representasjonene fungerer som feature-vektorer for hvert bildepunkt og brukes som input til den logistiske modellen.

Modellfilen dinov3_vits16.pth inneholder de forhÃ¥ndstrente vektene og lastes inn direkte i notebooken.

### **3. Trening av logistisk regresjon**
Feature-vektorene fra DINOv3 brukes til Ã¥ trene en logistisk regresjon fra scikit-learn.
Modellen lÃ¦rer Ã¥ skille mellom forgrunn (verktÃ¸y) og bakgrunn basert pÃ¥ maskene.

NÃ¥r treningen er ferdig, kan modellen brukes til Ã¥ generere en segmentert maske for de samme bildene som ble brukt under trening.

### **4. Evaluering og visualisering**
Notebooken viser resultatene i form av bilder:

Originalbilde

Forgrunnssannsynlighet

Predikert binÃ¦r maske

Dette gjÃ¸r det mulig Ã¥ se hvordan modellen lÃ¦rer Ã¥ markere verktÃ¸yet i hvert bilde. 

### **5. (Valgfritt) Video-demo**
Koden kan ogsÃ¥ vise hvordan modellen fungerer pÃ¥ en video i sanntid (eller pÃ¥ en lagret videofil).
Dette er ment som en visuell demonstrasjon av segmentering, ikke som en test eller mÃ¥ling av nÃ¸yaktighet.

---

## ðŸ“¦ NÃ¸dvendige filer
| Fil                    | Beskrivelse                           |
| ---------------------- | ------------------------------------- |
| `dinov3_vits16.pth`    | ForhÃ¥ndstrente vekter fra Meta AI     |
| `Data/images/`         | Treningsbilder                        |
| `Data/labels/`         | Maskebilder laget med click_n_mask.py |
| `click_n_mask.py`      | Program for maskegenerering           |
| `sam_vit_b_01ec64.pth` | Modellfil brukt av SAM                |
| `Videoer/`             | Valgfri mappe for testvideoer         |

---

## â–¶ï¸ Hvordan kjÃ¸re prosjektet
1. Ã…pne notebooken foreground_segmentation.ipynb.

2. KjÃ¸r cellene i rekkefÃ¸lge:

   -Last bilder og masker

   -KjÃ¸r DINOv3-feature-ekstraksjon

   -Tren logistisk regresjon

   -Segmenter og vis resultatene

3. (Valgfritt) KjÃ¸r video-delen for Ã¥ vise sanntidssegmentering.

---

## ðŸ’¡ Forklaring av DINOv3-delen
DINOv3 (Self-Supervised Vision Transformer) er trent av Meta AI uten manuelle labels.
Den lÃ¦rer Ã¥ forstÃ¥ visuelle mÃ¸nstre ved Ã¥ sammenligne ulike utsnitt av samme bilde.
Vi bruker denne modellen til Ã¥ hente meningsfulle features som kan brukes videre i klassifikasjon eller segmentering

---

## ðŸ‘¨â€ðŸ« Om prosjektet
- **Fag:** Anvendt MaskinlÃ¦ring  
- **Skole:** Viken Fagskole  
- **Student:** Ahmad Rezae  
- **Ã…r:** 2025  
- **Veileder:** Shahin Kamil Ostadahmadi 

---

> ðŸ“Œ Merk: Notebooken (`foreground_segmentation.ipynb`) er stÃ¸rre enn 20 MB, og GitHub kan derfor ikke vise den direkte i nettleseren.  
> For Ã¥ se innholdet, last ned repoet som ZIP og Ã¥pne notebooken lokalt i Jupyter Notebook.


## ðŸ“œ Kilder
- Meta AI Research â€“ [DINOv3: Self-supervised learning with Vision Transformers](https://github.com/facebookresearch/dinov3)  
- Scikit-Learn dokumentasjon: [https://scikit-learn.org](https://scikit-learn.org)  
- PyTorch dokumentasjon: [https://pytorch.org](https://pytorch.org)

---

ðŸ§© *Prosjektet viser hvordan man kan kombinere ferdigtrente nevrale nettverk med enkle klassifikasjonsmodeller for Ã¥ lÃ¸se komplekse oppgaver som bilde-segmentering pÃ¥ en effektiv mÃ¥te.*
